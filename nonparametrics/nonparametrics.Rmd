---
title: "Non-parametric stats"
output: 
  ioslides_presentation
---

## Non Parametric Stats

### when your data don't meet the assumptions

```{r echo=FALSE}
qqnorm(rnorm(1000)^3, main="(ab)normal Q-Q plot")
```

## WARNING - LOW POWER ZONE

> - If your data meet (or approximate) assumptions of parametrics, they are generally more powerful

> - Monte-Carlo techniques are also often more powerful than non-parametrics

> - However, non-parametrics simpler to use than MC

## Rank-Order Statistics


> - Non-parametrics are known as rank-order tests, because they work by ranking observations and analyzing these ranks, rather than the data themselves.

> - To use non-parametrics with continuous values, you have to discard a lot of information.

> - We will talk about non-parametrics in relation to their parametric equivalents. 

## Non-Parametric Regression


> - Non-parametric regression techniques exist but are not commonly used.

> - There are, however, several non-parametric correlation techniques that are widely used.

## Spearman's Rho



X and Y values are ranked separately, and the Pearson's product-moment coefficient ($r$) is computed on these ranks

```{r}
x <- c(0.9, 6.8, 3.2, 2.4, 1.2, 1.1)
y <- c(0.1, 4.5, 5.4, 1.5, 1.9, 4.1)
```

## Spearman's Rho


```{r}
rank_x <- rank(x)
rank_y <- rank(y)
rank_x
rank_y

```

## Spearman's Rho {.smaller}


```{r echo=FALSE, fig.height=4}
  par(mfrow=c(1,2))
plot(x, y, pch=16, main="Raw Data")
plot(rank_x,rank_y, pch=16, main="Ranked Data")
```

## Spearman's Rho

```{r}
cor(x, y, method = "pearson")
cor(x, y, method = "spearman")
cor(rank_x, rank_y, method = "pearson")
```

## Kendall's Tau


Alternative to Spearman....

> - Rank observations
> - Examine each pair of observations, determine whether they match or not
> - Compute $\tau$

> - $$\tau = \frac{(number\ of\ matched\ pairs) - (number\ of \ non\ matched\ pairs)}{\frac{1}{2}n(n-1)}$$

> - Note: the denominator is the total number of pairwise comparisons.

## Kendall's Tau



```{r echo=FALSE, fig.height=4}
var1 <- rnorm(10, mean=10); 
var2 <- var1 * 0.7 + rnorm(10, sd=0.2) 
ggplot2::qplot(var1, var2, size=I(4)) + ggplot2::theme_bw(20)
```


```{r}
cor(var1, var2, method="kendall")
```

## Non-Parametric t-test


Mann-Whitney U, also known as the Wilcoxon Rank-Sum

> - rank observations, ignoring group
> - sum the ranks belonging to each group
> - calculate the test statistic

> - $$U = R - \frac{n(n+1)}{2}$$

> - $R$ is the summed ranks, and $n$ is the group sample size
> - do this for both groups, and take the smallest as the test statistic
> - compare to known distribution under null hypothesis

## Mann-Whitney U / Wilcoxon Rank-Sum


```{r}
x <- rnorm(10, mean=5)
y <- rnorm(10, mean=7)
wilcox.test(x,y)
```

## Non-Parametric ANOVA


Kruskal-Wallis

> - Rank all observations
> - Calculate the average rank within each group
> - Compare the average rank within group the the overall average of ranks, using a weighted sum-of-squares technique
> - Compare p value of test statistic using chi-square approximation

## Kruskal-Wallis



```{r echo=F}
var <- c(rnorm(20, mean=6), rnorm(20, mean=5)) 
group <- factor(c(rep("A", 20), rep("B", 20)))
ggplot2::qplot(y=var, x=group, geom="boxplot", fill=group) + 
  ggplot2::theme_bw(30)
```

## Kruskal-Wallis

```{r}
kruskal.test(var~group)
```

## Goodness of Fit Test



Kolmogorov-Smirnov Test

Non-parametric test to determine whether two distributions differ

## Based on theoretical vs empirical CDF



```{r echo=FALSE}
par(mfrow=c(1,2))
x <- rnorm(1000)
y <- x * 0.3 + rnorm(1000, sd=0.1)
resids <- resid(lm(y^2~x))
plot(ecdf(resids), main="Empirical CDF")

#trying to figure out how to get it on the same plot
#xrange <- seq(from = min(resids), to=max(resids), length.out = 100)
#densities <- pnorm(q = xrange , sd = sd(resids))
#points(x=xrange, y=densities)

plot(ecdf(rnorm(1000, sd=sd(resids))), main="Theoretical EDF")
```

## KS-Test

> - The single largest deviation of the empirical from the theoretical is the KS statistic. This is used to compute a p-value.

> - Can be used for any distribution, not just the normal distribution.

## KS-Test in R

```{r}
ks.test(rnorm(100), "punif")

```

## KS-Test in R

```{r}
ks.test(rnorm(100)^2, "pnorm")
```

## Two Sample KS-Test in R


```{r}
ks.test(runif(100), rnorm(100))
```

